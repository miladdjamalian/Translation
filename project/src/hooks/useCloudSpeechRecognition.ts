import { useState, useRef, useCallback, useEffect } from 'react';
import { API_ENDPOINTS, apiRequest, TIMEOUTS } from '../config/api';

export const useCloudSpeechRecognition = (language: string, provider: 'google' | 'azure' = 'google') => {
  const [transcript, setTranscript] = useState('');
  const [interimTranscript, setInterimTranscript] = useState('');
  const [confidence, setConfidence] = useState(0);
  const [isListening, setIsListening] = useState(false);
  const [isProcessing, setIsProcessing] = useState(false);

  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioChunksRef = useRef<Blob[]>([]);
  const streamRef = useRef<MediaStream | null>(null);
  const processingTimeoutRef = useRef<NodeJS.Timeout>();
  const sessionIdRef = useRef<string>('');
  const accumulatedTranscriptRef = useRef<string>('');
  const isProcessingRef = useRef<boolean>(false);

  const getLanguageCode = (lang: string) => {
    const languageCodes: Record<string, string> = {
      'auto': 'fa-IR',
      'en': 'en-US',
      'fa': 'fa-IR',
      'ar': 'ar-SA',
      'es': 'es-ES',
      'fr': 'fr-FR',
      'de': 'de-DE',
      'it': 'it-IT',
      'pt': 'pt-PT',
      'ru': 'ru-RU',
      'ja': 'ja-JP',
      'ko': 'ko-KR',
      'zh': 'zh-CN',
      'hi': 'hi-IN',
      'th': 'th-TH',
      'vi': 'vi-VN',
      'nl': 'nl-NL',
      'sv': 'sv-SE',
      'no': 'no-NO',
      'da': 'da-DK',
      'fi': 'fi-FI',
      'tr': 'tr-TR',
      'pl': 'pl-PL',
      'cs': 'cs-CZ',
      'hu': 'hu-HU',
      'ro': 'ro-RO',
      'bg': 'bg-BG',
      'hr': 'hr-HR',
      'sk': 'sk-SK',
      'sl': 'sl-SI',
      'et': 'et-EE',
      'lv': 'lv-LV',
      'lt': 'lt-LT',
      'he': 'he-IL',
      'ur': 'ur-PK',
      'bn': 'bn-BD',
      'ta': 'ta-IN',
      'te': 'te-IN',
      'ml': 'ml-IN',
      'kn': 'kn-IN',
      'gu': 'gu-IN',
      'pa': 'pa-IN',
      'mr': 'mr-IN',
      'ne': 'ne-NP',
      'si': 'si-LK',
      'my': 'my-MM',
      'km': 'km-KH',
      'lo': 'lo-LA',
      'ka': 'ka-GE',
      'am': 'am-ET',
      'sw': 'sw-TZ',
      'zu': 'zu-ZA',
      'af': 'af-ZA',
      'xh': 'xh-ZA'
    };
    
    return languageCodes[lang] || 'fa-IR';
  };

  const processAudioChunk = useCallback(async (audioBlob: Blob) => {
    if (!audioBlob || audioBlob.size === 0) {
      console.log('âš ï¸ Empty audio blob, skipping'); // Debug log
      return;
    }

    if (isProcessingRef.current) {
      console.log('âš ï¸ Already processing, skipping chunk'); // Debug log
      return;
    }

    isProcessingRef.current = true;
    setIsProcessing(true);
    
    try {
      console.log('ðŸ“¤ Sending audio chunk to backend:', audioBlob.size, 'bytes, provider:', provider, 'language:', getLanguageCode(language)); // Debug log
      
      const formData = new FormData();
      formData.append('audio', audioBlob, 'audio.webm');
      formData.append('provider', provider);
      formData.append('language', getLanguageCode(language));
      formData.append('audioFormat', 'webm');

      const response = await apiRequest(API_ENDPOINTS.SPEECH_TRANSCRIBE, {
        method: 'POST',
        body: formData,
        signal: AbortSignal.timeout(TIMEOUTS.SPEECH_REQUEST)
      });

      console.log('ðŸ“¥ Speech API response status:', response.status, response.ok); // Debug log

      if (!response.ok) {
        const errorText = await response.text();
        console.error('âŒ Speech API error response:', errorText); // Debug log
        throw new Error(`Speech API error: ${response.statusText} - ${errorText}`);
      }

      const result = await response.json();
      console.log('ðŸ“‹ Speech API result:', result); // Debug log
      
      if (result.transcript && result.transcript.trim()) {
        console.log(`âœ… ${provider} Speech result:`, result.transcript);
        
        // Accumulate transcript instead of replacing
        const newText = result.transcript.trim();
        accumulatedTranscriptRef.current += (accumulatedTranscriptRef.current ? ' ' : '') + newText;
        setTranscript(accumulatedTranscriptRef.current);
        setConfidence(result.confidence || 0);
        setInterimTranscript('');
      } else {
        console.log('âš ï¸ No transcript in result or empty transcript'); // Debug log
      }

    } catch (error) {
      console.error(`âŒ ${provider} Speech API error:`, error);
    } finally {
      setIsProcessing(false);
      isProcessingRef.current = false;
    }
  }, [language, provider]);

  const startListening = useCallback(async () => {
    try {
      console.log('ðŸŽ¤ Starting cloud speech recognition for language:', getLanguageCode(language), 'provider:', provider); // Debug log
      
      // âœ… ØªØºÛŒÛŒØ± Ú©Ù„ÛŒØ¯ÛŒ: Ù†Ø±Ø® Ù†Ù…ÙˆÙ†Ù‡â€ŒØ¨Ø±Ø¯Ø§Ø±ÛŒ Ø±Ø§ Ø¨Ù‡ 48000 Ù‡Ø±ØªØ² ØªØºÛŒÛŒØ± Ø¯Ø§Ø¯ÛŒÙ…
      // Ø§ÛŒÙ† ØªØºÛŒÛŒØ± ØªØ¶Ù…ÛŒÙ† Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú©Ù‡ Ù†Ø±Ø® Ù†Ù…ÙˆÙ†Ù‡â€ŒØ¨Ø±Ø¯Ø§Ø±ÛŒ Ù…ÛŒÚ©Ø±ÙˆÙÙˆÙ† Ø¨Ø§ Ú©Ø¯Ú© Opus Ù…Ø·Ø§Ø¨Ù‚Øª Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          sampleRate: 48000, // ðŸ”§ ØªØºÛŒÛŒØ± Ø§Ø² 16000 Ø¨Ù‡ 48000 Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ø¨Ø§ Opus
          channelCount: 1 // ØªÚ© Ú©Ø§Ù†Ø§Ù„ Ø¨Ø±Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø­Ø¬Ù…
        }
      });
      
      console.log('âœ… Got media stream for speech recognition with 48kHz sample rate:', stream); // Debug log
      
      streamRef.current = stream;
      sessionIdRef.current = Date.now().toString();
      accumulatedTranscriptRef.current = '';
      
      // Check supported MIME types
      const supportedTypes = [
        'audio/webm;codecs=opus',
        'audio/webm',
        'audio/mp4',
        'audio/wav'
      ];
      
      let mimeType = 'audio/webm';
      for (const type of supportedTypes) {
        if (MediaRecorder.isTypeSupported(type)) {
          mimeType = type;
          break;
        }
      }
      
      console.log('ðŸŽµ Using MIME type for speech recognition:', mimeType); // Debug log
      
      const mediaRecorder = new MediaRecorder(stream, {
        mimeType: mimeType
      });
      
      mediaRecorderRef.current = mediaRecorder;
      audioChunksRef.current = [];
      
      // Process audio in chunks for real-time experience
      mediaRecorder.ondataavailable = (event) => {
        console.log('ðŸ“Š MediaRecorder data available:', event.data.size, 'bytes'); // Debug log
        
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data);
          
          // Process chunk immediately for real-time experience
          const audioBlob = new Blob([event.data], { type: mimeType });
          console.log('ðŸ”„ Processing audio chunk:', audioBlob.size, 'bytes'); // Debug log
          processAudioChunk(audioBlob);
        } else {
          console.log('âš ï¸ Received empty data chunk'); // Debug log
        }
      };
      
      mediaRecorder.onstart = () => {
        setIsListening(true);
        setTranscript('');
        setInterimTranscript('');
        accumulatedTranscriptRef.current = '';
        console.log(`ðŸ”´ ${provider} Speech recognition started with 48kHz sample rate`);
      };
      
      mediaRecorder.onstop = () => {
        setIsListening(false);
        console.log(`ðŸ›‘ ${provider} Speech recognition stopped`);
      };
      
      mediaRecorder.onerror = (event) => {
        console.error('âŒ MediaRecorder error:', event); // Debug log
        setIsListening(false);
      };
      
      // Start recording with smaller time slices for better real-time processing
      console.log('ðŸ”´ Starting MediaRecorder with 1000ms chunks'); // Debug log
      mediaRecorder.start(1000); // 1000ms chunks for better processing
      
    } catch (error) {
      console.error('âŒ Failed to start cloud speech recognition:', error);
      throw error;
    }
  }, [language, provider, processAudioChunk]);

  const stopListening = useCallback(() => {
    console.log('ðŸ›‘ Stopping cloud speech recognition'); // Debug log
    
    if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {
      mediaRecorderRef.current.stop();
    }
    
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => {
        track.stop();
        console.log('ðŸ”‡ Speech recognition track stopped:', track.label); // Debug log
      });
    }
    
    if (processingTimeoutRef.current) {
      clearTimeout(processingTimeoutRef.current);
    }
    
    setIsListening(false);
    setInterimTranscript('');
    isProcessingRef.current = false;
  }, []);

  // Cleanup on unmount
  useEffect(() => {
    return () => {
      stopListening();
    };
  }, [stopListening]);

  return {
    transcript,
    interimTranscript,
    confidence,
    isListening,
    isProcessing,
    startListening,
    stopListening
  };
};
